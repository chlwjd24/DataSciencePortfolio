y_train <- train[,3]
y_test <- test[,3]
y_pred <- logistic_regression(X_train, y_train, X_test, 0.1, 1000, threshold=0.00001, print=True)
df <- read.csv("C:/Users/brian/Google Drive/uni/STAT8178/ass1/BinaryClassifier.csv", header=FALSE)
colnames(df) <- c("x1","x2","y")
train <- df[1:456,]
test <- df[457:569,]
plot(train, test)
model <- glm(formula = y ~ x1 + x2, family = "binomial", data = train)
library(dplyr)
library(caret)
library(ggplot2)
test$prob <- predict(model, test, type = "response")
test <- test %>% mutate(pred = ifelse(prob>.5, "1", "0"))
confusionMatrix(factor(test$pred),factor(test$y), mode="everything")
sigmoid<-function(a){
return(1/(1+exp(-a)))
}
prediction <- function(x, w, classify=FALSE){
a <- sigmoid(w%*%x)
if(classify){
return(a>0.5)
}
else{
return(a)
}
}
cross_entropy<-function(y_true,y_pred){
num <- length(y_true)
y_true<- as.array(y_true)
y_pred<-as.array(y_pred)
total<--sum(y_true*log(y_pred)+(1-y_true)*log(1-y_pred))
return(total/num)
}
gradient_descent <- function(alpha, epoch, weight, X, y, max, print=TRUE, cost=FALSE) {
y_pred <- sapply(X, prediction, w=weight)
num <- length(y)
cost <- c()
for (i in 1:epoch) {
dw <- sum((y_pred - y) * X) / num
weight <- weight - alpha * dw
y_pred <- sapply(X, prediction, w=weight)
new_cost <- cross_entropy(y, y_pred)
cost <- c(cost, new_cost)
if (print && i %% 50 == 0) {
print(paste("Iteration", i, ", Cost:", new_cost))
}
if (i > 3 && cost[length(cost) - 1] - cost[length(cost)] < max) {
break
}
}
if (cost) {
return(cost)
} else {
return(weight)
}
}
logistic_regression <- function(train_set, y, test_set, alpha, epoch, threshold=0.0001, print=FALSE, cost=FALSE) {
weight <- runif(length(train_set[1]))
if (cost) {
cost <- gradient_descent(alpha, epoch, weight, train_set, y, threshold, print, cost)
return(cost)
} else {
new_weight <- gradient_descent(alpha, epoch, weight, train_set, y, threshold, print)
prediction <- sapply(test_set, function(instance) make_prediction(new_weight, instance, classify=TRUE))
return(as.array(prediction))
}
}
X_train <- train[,1:2]
X_test <- test[,1:2]
y_train <- train[,3]
y_test <- test[,3]
y_pred <- logistic_regression(X_train, y_train, X_test, 0.1, 1000, threshold=0.00001, print=True)
sigmoid<-function(a){
return(1/(1+exp(-a)))
}
prediction <- function(x, w, classify=FALSE){
a <- sigmoid(w%*%x)
if(classify){
return(a>0.5)
}
else{
return(a)
}
}
cross_entropy<-function(y_true,y_pred){
num <- length(y_true)
y_true<- as.array(y_true)
y_pred<-as.array(y_pred)
total<-sum(y_true*log(y_pred)+(1-y_true)*log(1-y_pred))
return(total/num)
}
gradient_descent <- function(alpha, epoch, weight, X, y, max, print=TRUE, cost=FALSE) {
y_pred <- sapply(X, prediction, w=weight)
num <- length(y)
cost <- c()
for (i in 1:epoch) {
dw <- sum((y_pred - y) * X) / num
weight <- weight - alpha * dw
y_pred <- sapply(X, prediction, w=weight)
new_cost <- cross_entropy(y, y_pred)
cost <- c(cost, new_cost)
if (print && i %% 50 == 0) {
print(paste("Iteration", i, ", Cost:", new_cost))
}
if (i > 3 && cost[length(cost) - 1] - cost[length(cost)] < max) {
break
}
}
if (cost) {
return(cost)
} else {
return(weight)
}
}
logistic_regression <- function(train_set, y, test_set, alpha, epoch, threshold=0.0001, print=FALSE, cost=FALSE) {
weight <- runif(length(train_set[1]))
if (cost) {
cost <- gradient_descent(alpha, epoch, weight, train_set, y, threshold, print, cost)
return(cost)
} else {
new_weight <- gradient_descent(alpha, epoch, weight, train_set, y, threshold, print)
prediction <- sapply(test_set, function(instance) make_prediction(new_weight, instance, classify=TRUE))
return(as.array(prediction))
}
}
X_train <- train[,1:2]
X_test <- test[,1:2]
y_train <- train[,3]
y_test <- test[,3]
y_pred <- logistic_regression(X_train, y_train, X_test, 0.1, 1000, threshold=0.00001, print=True)
logistic_regression <- function(training_set, label, test_set, alpha, epoch, threshold = 0.0001, print_option = FALSE, get_cost = FALSE) {
weight <- runif(length(training_set[1]))
if (get_cost) {
cost <- gradient_descent(alpha, epoch, weight, training_set, label, threshold, print_option, get_cost)
return (cost)
} else {
new_weight <- gradient_descent(alpha, epoch, weight, training_set, label, threshold, print_option)
prediction <- sapply(test_set, function(instance) make_prediction(new_weight, instance, classify = TRUE))
return (as.integer(prediction))
}
}
X_train <- train[,1:2]
X_test <- test[,1:2]
y_train <- train[,3]
y_test <- test[,3]
y_pred <- logistic_regression(X_train, y_train, X_test, 0.1, 1000, threshold=0.00001, print=True)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(ggplot2)
sigmoid <- function(x) {
return (1 / (1 + exp(-x)))
}
make_prediction <- function(w, x, classify = FALSE) {
z <- sigmoid(sum(w * x))
if (classify) {
return (as.integer(z > 0.5))
} else {
return (z)
}
}
cross_entropy <- function(y_true, y_pred) {
data_num <- length(y_true)
y_true <- as.numeric(y_true)
y_pred <- as.numeric(y_pred)
total <- -sum(y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))
return (total / data_num)
}
gradient_descent <- function(alpha, epoch, weight, X, y, threshold, print_option = TRUE, get_cost = FALSE) {
y_pred <- sapply(X, function(x) make_prediction(weight, x))
data_num <- length(y)
cost <- c()
for (i in 1:epoch) {
dw <- sum((y_pred - y) * X) / data_num
weight <- weight - alpha * dw
y_pred <- sapply(X, function(x) make_prediction(weight, x))
new_cost <- cross_entropy(y, y_pred)
cost <- c(cost, new_cost)
if (print_option && i %% 50 == 0) {
cat(sprintf("Iteration %d, Cost: %f\n", i, new_cost))
}
if (i > 3 && cost[length(cost) - 1] - cost[length(cost)] < threshold) {
break
}
}
if (get_cost) {
return (cost)
} else {
return (weight)
}
}
logistic_regression <- function(training_set, label, test_set, alpha, epoch, threshold = 0.0001, print_option = FALSE, get_cost = FALSE) {
weight <- runif(length(training_set[1]))
if (get_cost) {
cost <- gradient_descent(alpha, epoch, weight, training_set, label, threshold, print_option, get_cost)
return (cost)
} else {
new_weight <- gradient_descent(alpha, epoch, weight, training_set, label, threshold, print_option)
prediction <- sapply(test_set, function(instance) make_prediction(new_weight, instance, classify = TRUE))
return (as.integer(prediction))
}
}
accuracy_score <- function(y_true, y_pred) {
count <- 0
for (i in 1:length(y_true)) {
if (y_true[i] == y_pred[i]) {
count <- count + 1
}
}
return (count / length(y_true))
}
plot_accuracy <- function(alpha, epoch, X_train, y_train, X_test, y_test) {
accuracy <- c()
iter_range <- seq(1, epoch)
for (iter_num in iter_range) {
y_hat <- logistic_regression(X_train, y_train, X_test, alpha, iter_num)
accuracy <- c(accuracy, accuracy_score(y_hat, y_test))
}
plot(iter_range, accuracy, type = "l", col = "skyblue", xlab = "Epochs", ylab = "Accuracy")
}
y_pred <- logistic_regression(X_train, y_train, X_test, 0.1, 1000, threshold=0.00001, print=True)
y_pred <- logistic_regression(X_train, y_train, X_test, 0.1, 1000, threshold=0.00001, print)
y_pred <- logistic_regression(X_train, y_train, X_test, 0.1, 1000, threshold=0.00001, print)
gradient_descent(0.1, 200, x_train, np.random.rand(len(training_set[0])) y_train, 0.0001)
gradient_descent(0.1, 200, x_train,runif(length(training_set[[1]]))
, y_train, 0.0001)
gradient_descent(0.1, 200, x_train,runif(length(x_train[[1]]))
, y_train, 0.0001)
gradient_descent(0.1, 200, x_train,runif(length(X_train[[1]]))
, y_train, 0.0001)
gradient_descent(0.1, 200, X_train,runif(length(X_train[[1]]))
, y_train, 0.0001)
library(dplyr)
library(caret)
library(ggplot2)
sigmoid <- function(x) {
return (1 / (1 + exp(-x)))
}
make_prediction <- function(w, x, classify = FALSE) {
z <- sigmoid(sum(w * x))
if (classify) {
return (as.integer(z > 0.5))
} else {
return (z)
}
}
cross_entropy <- function(y_true, y_pred) {
data_num <- length(y_true)
y_true <- as.numeric(y_true)
y_pred <- as.numeric(y_pred)
total <- -sum(y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))
return (total / data_num)
}
gradient_descent <- function(alpha, epoch, weight, X, y, threshold, print_option = TRUE, get_cost = FALSE) {
y_pred <- sapply(X, function(x) make_prediction(weight, x))
data_num <- length(y)
cost <- c()
for (i in 1:epoch) {
dw <- sum((y_pred - y) * X) / data_num
weight <- weight - alpha * dw
y_pred <- sapply(X, function(x) make_prediction(weight, x))
new_cost <- cross_entropy(y, y_pred)
cost <- c(cost, new_cost)
if (print_option && i %% 50 == 0) {
cat(sprintf("Iteration %d, Cost: %f\n", i, new_cost))
}
if ((i > 3 && cost[length(cost) - 1] - cost[length(cost)]) < threshold) {
break
}
}
if (get_cost) {
return (cost)
} else {
return (weight)
}
}
logistic_regression <- function(training_set, label, test_set, alpha, epoch, threshold = 0.0001, print_option = FALSE, get_cost = FALSE) {
weight <- runif(length(training_set[1]))
if (get_cost) {
cost <- gradient_descent(alpha, epoch, weight, training_set, label, threshold, print_option, get_cost)
return (cost)
} else {
new_weight <- gradient_descent(alpha, epoch, weight, training_set, label, threshold, print_option)
prediction <- sapply(test_set, function(instance) make_prediction(new_weight, instance, classify = TRUE))
return (as.integer(prediction))
}
}
accuracy_score <- function(y_true, y_pred) {
count <- 0
for (i in 1:length(y_true)) {
if (y_true[i] == y_pred[i]) {
count <- count + 1
}
}
return (count / length(y_true))
}
plot_accuracy <- function(alpha, epoch, X_train, y_train, X_test, y_test) {
accuracy <- c()
iter_range <- seq(1, epoch)
for (iter_num in iter_range) {
y_hat <- logistic_regression(X_train, y_train, X_test, alpha, iter_num)
accuracy <- c(accuracy, accuracy_score(y_hat, y_test))
}
plot(iter_range, accuracy, type = "l", col = "skyblue", xlab = "Epochs", ylab = "Accuracy")
}
gradient_descent(0.1, 200, X_train,runif(length(X_train[[1]]))
, y_train, 0.0001)
y_pred = logistic_regression(X_train, y_train, X_test, 0.1, 1000, threshold=0.00001, print_option=True)
y_pred = logistic_regression(X_train, y_train, X_test, 0.1, 1000, threshold=0.00001, print_option=TRUE)
y_pred = logistic_regression(X_train, y_train, X_test, 0.1, 1000, threshold=0.00001, print_option=TRUE)
y_pred = logistic_regression(X_train, y_train, X_test, 0.1, 1000, threshold=0.00001, print_option=TRUE)
y_pred <- logistic_regression(X_train, y_train, X_test, 0.1, 1000, threshold=0.00001, print_option=TRUE)
sigmoid<-function(a){
return(1/(1+exp(-a)))
}
make_prediction <- function(w, x, classify = FALSE) {
z <- sigmoid(sum(w * x))
if (classify) {
return (as.integer(z > 0.5))
} else {
return (z)
}
}
cross_entropy <- function(y_true, y_pred) {
data_num <- length(y_true)
y_true <- as.numeric(y_true)
y_pred <- as.numeric(y_pred)
total <- -sum(y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))
return (total / data_num)
}
gradient_descent <- function(alpha, epoch, weight, X, y, threshold, print_option = TRUE, get_cost = FALSE) {
y_pred <- sapply(X, function(x) make_prediction(weight, x))
data_num <- length(y)
cost <- c()
for (i in 1:epoch) {
dw <- sum((y_pred - y) * X) / data_num
weight <- weight - alpha * dw
y_pred <- sapply(X, function(x) make_prediction(weight, x))
new_cost <- cross_entropy(y, y_pred)
cost <- c(cost, new_cost)
if (print_option && i %% 50 == 0) {
cat(sprintf("Iteration %d, Cost: %f\n", i, new_cost))
}
if ((i > 3 && cost[length(cost) - 1] - cost[length(cost)]) < threshold) {
break
}
}
if (get_cost) {
return (cost)
} else {
return (weight)
}
}
logistic_regression <- function(training_set, label, test_set, alpha, epoch, threshold = 0.0001, print_option = FALSE, get_cost = FALSE) {
weight <- runif(length(training_set[1]))
if (get_cost) {
cost <- gradient_descent(alpha, epoch, weight, training_set, label, threshold, print_option, get_cost)
return (cost)
} else {
new_weight <- gradient_descent(alpha, epoch, weight, training_set, label, threshold, print_option)
prediction <- sapply(test_set, function(instance) make_prediction(new_weight, instance, classify = TRUE))
return (as.integer(prediction))
}
}
accuracy_score <- function(y_true, y_pred) {
count <- 0
for (i in 1:length(y_true)) {
if (y_true[i] == y_pred[i]) {
count <- count + 1
}
}
return (count / length(y_true))
}
plot_accuracy <- function(alpha, epoch, X_train, y_train, X_test, y_test) {
accuracy <- c()
iter_range <- seq(1, epoch)
for (iter_num in iter_range) {
y_hat <- logistic_regression(X_train, y_train, X_test, alpha, iter_num)
accuracy <- c(accuracy, accuracy_score(y_hat, y_test))
}
plot(iter_range, accuracy, type = "l", col = "skyblue", xlab = "Epochs", ylab = "Accuracy")
}
X_train <- train[,1:2]
X_test <- test[,1:2]
y_train <- train[,3]
y_test <- test[,3]
y_pred <- logistic_regression(X_train, y_train, X_test, 0.1, 1000, threshold=0.00001, print=True)
y_pred <- logistic_regression(X_train, y_train, X_test, 0.1, 1000, threshold=0.00001, print=TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(plot3D)
x = cbind(c(1,1,1,1,1,1),c(-3,-2,-1,1,2,3))
y = c(1,0,0,1,1,0)
sigmoid <- function(x){
sigmoid <- 1 / (1 + exp(-x))
}
w1_vals = seq(-10, 10, length.out = 100)
w2_vals = seq(-5, 5, length.out = 50)
w1_grid = outer(w1_vals, w2_vals, FUN = "*")
w2_grid = outer(w1_vals, w2_vals, FUN = "*")
loss_grid <- matrix(0, nrow = length(w1_vals), ncol = length(w2_vals))
for (i in 1:length(w1_vals)) {
for (j in 1:length(w2_vals)) {
w <- c(w1_vals[i], w2_vals[j])
loss_grid[i, j] <- mean(-y*log(sigmoid(x%*%w)) - (1-y) * log(1-sigmoid(x%*%w)))
}
}
p <- persp3D(w1_vals,w2_vals,loss_grid,theta=45, phi=30, axes=TRUE,scale=2, box=TRUE, nticks=5,
ticktype="detailed", xlab="", ylab="", zlab="")
p
loss_grid <- matrix(0, nrow = length(w1_vals), ncol = length(w2_vals))
for (i in 1:length(w1_vals)) {
for (j in 1:length(w2_vals)) {
w <- c(w1_vals[i], w2_vals[j])
loss_grid[i, j] <- mean(((sigmoid(x%*%w)) - y)^2)
}
}
p <- persp3D(w1_vals,w2_vals,loss_grid,theta=45, phi=30, axes=TRUE,scale=2, box=TRUE, nticks=5,
ticktype="detailed", xlab="", ylab="", zlab="")
p
df <- read.csv("C:/Users/brian/Google Drive/uni/STAT8178/ass1/BinaryClassifier.csv", header=FALSE)
colnames(df) <- c("x1","x2","y")
train <- df[1:456,]
test <- df[457:569,]
plot(train, test)
model <- glm(formula = y ~ x1 + x2, family = "binomial", data = train)
library(dplyr)
library(caret)
library(ggplot2)
test$prob <- predict(model, test, type = "response")
test <- test %>% mutate(pred = ifelse(prob>.5, "1", "0"))
confusionMatrix(factor(test$pred),factor(test$y), mode="everything")
sigmoid<-function(a){
return(1/(1+exp(-a)))
}
make_prediction <- function(w, x, classify = FALSE) {
z <- sigmoid(sum(w * x))
if (classify) {
return (as.integer(z > 0.5))
} else {
return (z)
}
}
cross_entropy <- function(y_true, y_pred) {
data_num <- length(y_true)
y_true <- as.numeric(y_true)
y_pred <- as.numeric(y_pred)
total <- -sum(y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))
return (total / data_num)
}
gradient_descent <- function(alpha, epoch, weight, X, y, threshold, print_option = TRUE, get_cost = FALSE) {
y_pred <- sapply(X, function(x) make_prediction(weight, x))
data_num <- length(y)
cost <- c()
for (i in 1:epoch) {
dw <- sum((y_pred - y) * X) / data_num
weight <- weight - alpha * dw
y_pred <- sapply(X, function(x) make_prediction(weight, x))
new_cost <- cross_entropy(y, y_pred)
cost <- c(cost, new_cost)
if (print_option && i %% 50 == 0) {
cat(sprintf("Iteration %d, Cost: %f\n", i, new_cost))
}
if ((i > 3 && cost[length(cost) - 1] - cost[length(cost)]) < threshold) {
break
}
}
if (get_cost) {
return (cost)
} else {
return (weight)
}
}
logistic_regression <- function(training_set, label, test_set, alpha, epoch, threshold = 0.0001, print_option = FALSE, get_cost = FALSE) {
weight <- runif(length(training_set[1]))
if (get_cost) {
cost <- gradient_descent(alpha, epoch, weight, training_set, label, threshold, print_option, get_cost)
return (cost)
} else {
new_weight <- gradient_descent(alpha, epoch, weight, training_set, label, threshold, print_option)
prediction <- sapply(test_set, function(instance) make_prediction(new_weight, instance, classify = TRUE))
return (as.integer(prediction))
}
}
X_train <- train[,1:2]
X_test <- test[,1:2]
y_train <- train[,3]
y_test <- test[,3]
y_pred <- logistic_regression(X_train, y_train, X_test, 0.1, 1000, threshold=0.00001, print=TRUE)
accuracy_score <- function(y_true, y_pred) {
count <- 0
for (i in 1:length(y_true)) {
if (y_true[i] == y_pred[i]) {
count <- count + 1
}
}
return (count / length(y_true))
}
plot_accuracy <- function(alpha, epoch, X_train, y_train, X_test, y_test) {
accuracy <- c()
iter_range <- seq(1, epoch)
for (iter_num in iter_range) {
y_hat <- logistic_regression(X_train, y_train, X_test, alpha, iter_num)
accuracy <- c(accuracy, accuracy_score(y_hat, y_test))
}
plot(iter_range, accuracy, type = "l", col = "skyblue", xlab = "Epochs", ylab = "Accuracy")
}
plot_accuracy(0.1, 200)
confusionMatrix(factor(test$pred),factor(test$y), mode="everything")
library(xfun)
remove.packages("xfun", lib="~/R/win-library/4.1")
install.packages("xfun")
