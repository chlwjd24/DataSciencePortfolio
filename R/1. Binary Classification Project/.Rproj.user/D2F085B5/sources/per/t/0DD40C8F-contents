---
title: "Assignment 1"
author: "Brian Choi"
date: "20/03/2023"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1
## Part A
$$\nabla C_1(w;Data)=-\frac{1}{m}\Sigma^m_{i=1} \nabla(y^{(i)}log(\hat{y}^{(i)})+(1-y^{(i)})log(1-\hat{y}^{(i)}))$$

$$=-\frac{1}{m}\Sigma^m_{i=1} \nabla y^{(i)}log(\sigma(w^Tx))+(1-y^{(i)})log(1-\sigma(w^Tx))$$
$$=\frac{1}{m}(\sigma(w^Tx^{(i)})-y^{(i)})x^{(i)}$$

## Part B
$$\nabla C_2(w;Data)=-\frac{1}{m}\Sigma^m_{i=1} \nabla(\sigma(w^Tx^{(i)}-y^{(i)})^2$$
$$=\frac{1}{m}(\sigma(W^Tx^{(i)})-y^{(i)})\sigma^\prime(w^Tx^{(i)})x^{(i)} $$

## Part C
$$H_1 =\nabla^2(C_1(w;Data))=\frac{1}{m}\sigma(w^Tx^{(i)})(1-\sigma(w^Tx^{(i)}))x^{(i)}(x^{(i)})^T$$

## Part D
$$H_2 =\nabla^2(C_1(w;Data))=\frac{1}{m}\sigma(w^Tx^{(i)})(1-\sigma(w^Tx^{(i)}))(x^{(i)})^T(x^{(i)})^T$$

## Part E]
To show that the function is convex, the following equation needs to hold true:
$$v^TH_1=\frac{1}{m}\sigma(w^Tx^{(i)})(1-\sigma(w^Tx^{(i)}))(v^Tx^{(i)})^2\ge0$$
Since the sigmoid function always has a value between 0 and 1 the equation is always positive semidefinite. Therefore, $C_1(w;Data)$ is convex. 

## Part F

$$H_2 =\nabla^2(C_1(w;Data))=\frac{1}{m}\sigma(w^Tx^{(i)})(1-\sigma(w^Tx^{(i)}))(x^{(i)})^T(x^{(i)})^T$$
Let $w=(0, 0)^T$ then,

$$H_2 = \frac{1}{m}(x^{(i)})^T(x^{(i)})^T$$
Considering the case where $w=(0,0)^T$, the matrix si not positive definite since the eigenvalues are approximately -0.114 and 24.114 according to the data given.

## Part G
Since $C_1(w;Data) is convex it should be used for training the model as it can be used for gradient descent optimisation techniques.

## Part H
```{r}
library(plot3D)

x = cbind(c(1,1,1,1,1,1),c(-3,-2,-1,1,2,3))
y = c(1,0,0,1,1,0)
sigmoid <- function(x){
  sigmoid <- 1 / (1 + exp(-x))
}

w1_vals = seq(-10, 10, length.out = 100)
w2_vals = seq(-5, 5, length.out = 50)
w1_grid = outer(w1_vals, w2_vals, FUN = "*")
w2_grid = outer(w1_vals, w2_vals, FUN = "*")

loss_grid <- matrix(0, nrow = length(w1_vals), ncol = length(w2_vals))

for (i in 1:length(w1_vals)) {
  for (j in 1:length(w2_vals)) {
    w <- c(w1_vals[i], w2_vals[j])
    loss_grid[i, j] <- mean(-y*log(sigmoid(x%*%w)) - (1-y) * log(1-sigmoid(x%*%w)))
  }
}

p <- persp3D(w1_vals,w2_vals,loss_grid,theta=45, phi=30, axes=TRUE,scale=2, box=TRUE, nticks=5,
        ticktype="detailed", xlab="", ylab="", zlab="")
p
```

## Part I
```{r}
loss_grid <- matrix(0, nrow = length(w1_vals), ncol = length(w2_vals))

for (i in 1:length(w1_vals)) {
  for (j in 1:length(w2_vals)) {
    w <- c(w1_vals[i], w2_vals[j])
    loss_grid[i, j] <- mean(((sigmoid(x%*%w)) - y)^2)
  }
}

p <- persp3D(w1_vals,w2_vals,loss_grid,theta=45, phi=30, axes=TRUE,scale=2, box=TRUE, nticks=5,
        ticktype="detailed", xlab="", ylab="", zlab="")
p
```

## Part J
The first plot shows a bowl shaped graph and has a single minimum whereas the second plot shows multiple global minima and therefore is not convex. Therefore, it is better to use the first loss function as there is less complexity and easier and more effective to optimise.

# Question 2
## Part A
```{r}
df <- read.csv("C:/Users/brian/Google Drive/uni/STAT8178/ass1/BinaryClassifier.csv", header=FALSE)
colnames(df) <- c("x1","x2","y")
train <- df[1:456,]
test <- df[457:569,]

plot(train, test)
```

The scatter plots of the train and test sets show that they are both randomly chosen. 


## Part B
```{r}
model <- glm(formula = y ~ x1 + x2, family = "binomial", data = train)
```
## Part C
```{r}
library(dplyr)
library(caret)
test$prob <- predict(model, test, type = "response")
test <- test %>% mutate(pred = ifelse(prob>.5, "1", "0"))
confusionMatrix(test$pred,test$y, mode="everything")

```

## Part D
```{r}

```
## Part E
```{r}

```
## Part F
```{r}

```
## Part G
```{r}

```